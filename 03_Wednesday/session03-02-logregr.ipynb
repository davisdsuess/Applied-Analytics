{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Block 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "0cb67764-53d0-4532-bd46-0d244615eb22",
    "_uuid": "d2792fa5793698bcab3aca4e99c5cba382edc042",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n...\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "'''\n",
    "...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset (You can test both preprocessed and raw data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f2ebf534-cd7d-40cb-94f1-d01d3b47854b",
    "_uuid": "7701a4dec64aac2cb2207d948daac81609fce014",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/df_imputed_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1714227-b678-4590-a1f3-65418ee42983",
    "_uuid": "68fff0bba19fa506cb14a6fa4ca838bc5a5e6d1a"
   },
   "source": [
    "### Let's Do Some Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f948a9ec-cf4c-4b0c-8255-cbfbbc3d9a58",
    "_uuid": "aa1227d23a4f5b53268ecb111604d1eaa9023f54"
   },
   "source": [
    "### Stratification:\n",
    "Split the dataset into train (75%) and test datasets (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: (546, 12) Testdaten: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratification: Datensatz in Training (75%) und Test (25%) aufteilen\n",
    "X = df.drop(columns=['Outcome'])  # Features (alle au√üer Zielvariable)\n",
    "y = df['Outcome']  # Zielvariable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Ges√§uberte Daten anzeigen\n",
    "print(\"Trainingsdaten:\", X_train.shape, \"Testdaten:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a Logistic Regression with test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: (546, 12) Testdaten: (183, 12)\n",
      "Genauigkeit des Modells: 0.7650273224043715\n",
      "Klassifikationsbericht:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.86      0.83       121\n",
      "         1.0       0.68      0.58      0.63        62\n",
      "\n",
      "    accuracy                           0.77       183\n",
      "   macro avg       0.74      0.72      0.73       183\n",
      "weighted avg       0.76      0.77      0.76       183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Stratification: Datensatz in Training (75%) und Test (25%) aufteilen\n",
    "X = df.drop(columns=['Outcome'])  # Features (alle au√üer Zielvariable)\n",
    "y = df['Outcome']  # Zielvariable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Logistische Regression\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluierung des Modells\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Trainingsdaten:\", X_train.shape, \"Testdaten:\", X_test.shape)\n",
    "print(\"Genauigkeit des Modells:\", accuracy)\n",
    "print(\"Klassifikationsbericht:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Odds Ratios \n",
    "Find out about the feature importance by calculating the odds ratios of the logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: (546, 12) Testdaten: (183, 12)\n",
      "Genauigkeit des Modells: 0.7595628415300546\n",
      "Klassifikationsbericht:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       121\n",
      "         1.0       0.68      0.55      0.61        62\n",
      "\n",
      "    accuracy                           0.76       183\n",
      "   macro avg       0.73      0.71      0.72       183\n",
      "weighted avg       0.75      0.76      0.75       183\n",
      "\n",
      "Feature Importance (Odds Ratios):\n",
      "                      Feature  Odds Ratio\n",
      "2                    Glucose   51.423750\n",
      "9       Mahalanobis_Distance    6.234325\n",
      "6                        BMI    5.920159\n",
      "7   DiabetesPedigreeFunction    4.390899\n",
      "1                Pregnancies    2.545815\n",
      "8                        Age    2.047151\n",
      "5                    Insulin    1.381729\n",
      "4              SkinThickness    1.332747\n",
      "10      Multivariate_Outlier    1.000000\n",
      "3              BloodPressure    0.978303\n",
      "0                 Unnamed: 0    0.936679\n",
      "11                   Outlier    0.763147\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Stratification: Datensatz in Training (75%) und Test (25%) aufteilen\n",
    "X = df.drop(columns=['Outcome'])  # Features (alle au√üer Zielvariable)\n",
    "y = df['Outcome']  # Zielvariable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Min-Max-Normalisierung\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Logistische Regression\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluierung des Modells\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Trainingsdaten:\", X_train.shape, \"Testdaten:\", X_test.shape)\n",
    "print(\"Genauigkeit des Modells:\", accuracy)\n",
    "print(\"Klassifikationsbericht:\\n\", report)\n",
    "\n",
    "# Berechnung der Odds Ratios\n",
    "odds_ratios = np.exp(model.coef_)[0]  # Exponentiierung der Koeffizienten\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Odds Ratio': odds_ratios\n",
    "}).sort_values(by='Odds Ratio', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Odds Ratios):\\n\", feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist die Interpretation des Outputs:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Modellleistung**\n",
    "- **Genauigkeit (Accuracy):**  \n",
    "  - Das Modell erreicht eine **Genauigkeit von 75,96%**.  \n",
    "  - Dies bedeutet, dass etwa 76% der Testf√§lle korrekt vorhergesagt wurden.\n",
    "\n",
    "- **Klassifikationsbericht:**  \n",
    "  - **Pr√§zision (Precision):**  \n",
    "    - **Klasse 0 (Kein Diabetes)**: 79% der als \"Kein Diabetes\" klassifizierten F√§lle sind korrekt.  \n",
    "    - **Klasse 1 (Diabetes)**: 68% der als \"Diabetes\" vorhergesagten F√§lle sind tats√§chlich Diabetes.  \n",
    "  - **Recall (Empfindlichkeit):**  \n",
    "    - **Klasse 0:** 87% der tats√§chlichen \"Kein Diabetes\"-F√§lle wurden korrekt erkannt.  \n",
    "    - **Klasse 1:** Nur 55% der tats√§chlichen Diabetes-F√§lle wurden erkannt.  \n",
    "  - **F1-Score:**  \n",
    "    - Das Modell erkennt **\"Kein Diabetes\" (F1-Score 83%) besser als \"Diabetes\" (F1-Score 61%)**.  \n",
    "    - Dies deutet darauf hin, dass das Modell eine **leichte Tendenz zur Vorhersage von \"Kein Diabetes\" hat**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Feature Importance ‚Äì Odds Ratios**\n",
    "Die **Odds Ratios (OR)** zeigen, wie stark sich die Odds f√ºr Diabetes √§ndern, wenn sich der Wert eines Merkmals um 1 Einheit erh√∂ht.\n",
    "\n",
    "- **Glucose (OR = 51.42)**  \n",
    "  - Das wichtigste Merkmal.  \n",
    "  - Eine Erh√∂hung des Glucose-Wertes erh√∂ht die Wahrscheinlichkeit f√ºr Diabetes **stark**.  \n",
    "  - Glucose ist somit der st√§rkste Pr√§diktor f√ºr Diabetes.\n",
    "\n",
    "- **Mahalanobis_Distance (OR = 6.23)**  \n",
    "  - H√∂here Werte dieses Features korrelieren mit einem erh√∂hten Diabetes-Risiko.  \n",
    "  - Mahalanobis-Distanz kann ein Indikator f√ºr abweichende Werte sein.\n",
    "\n",
    "- **BMI (OR = 5.92)**  \n",
    "  - Ein h√∂herer BMI erh√∂ht die Wahrscheinlichkeit f√ºr Diabetes signifikant.  \n",
    "\n",
    "- **DiabetesPedigreeFunction (OR = 4.39)**  \n",
    "  - Familienhistorie spielt eine wichtige Rolle bei Diabetes.\n",
    "\n",
    "- **Pregnancies (OR = 2.54)**  \n",
    "  - Mehr Schwangerschaften erh√∂hen das Risiko f√ºr Diabetes.\n",
    "\n",
    "- **Alter (OR = 2.05)**  \n",
    "  - √Ñltere Menschen haben ein h√∂heres Diabetes-Risiko.\n",
    "\n",
    "- **Insulin (OR = 1.38), SkinThickness (OR = 1.33)**  \n",
    "  - Diese Merkmale haben einen moderaten Einfluss auf Diabetes.\n",
    "\n",
    "- **BloodPressure (OR = 0.98)**  \n",
    "  - Kein starker Zusammenhang mit Diabetes.\n",
    "\n",
    "- **Outlier (OR = 0.76)**  \n",
    "  - Dieses Feature senkt m√∂glicherweise die Diabetes-Wahrscheinlichkeit oder wurde anders verarbeitet.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Fazit**\n",
    "- Das Modell ist **gut, aber k√∂nnte verbessert werden**, insbesondere bei der Erkennung von **Diabetes-F√§llen (Recall von 55%)**.\n",
    "- **Glucose ist der wichtigste Pr√§diktor f√ºr Diabetes**, gefolgt von **BMI, Mahalanobis_Distance und DiabetesPedigreeFunction**.\n",
    "- **BloodPressure hat kaum Einfluss auf die Odds f√ºr Diabetes**.\n",
    "- Eine bessere Balance zwischen Pr√§zision und Recall k√∂nnte helfen, mehr Diabetes-F√§lle richtig zu erkennen.\n",
    "\n",
    "Falls du Optimierungsm√∂glichkeiten brauchst, wie z. B. Hyperparameter-Tuning oder Feature Selection, lass es mich wissen! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: (546, 5) Testdaten: (183, 5)\n",
      "Genauigkeit des Modells: 0.7540983606557377\n",
      "Klassifikationsbericht:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       121\n",
      "         1.0       0.64      0.63      0.63        62\n",
      "\n",
      "    accuracy                           0.75       183\n",
      "   macro avg       0.73      0.72      0.72       183\n",
      "weighted avg       0.75      0.75      0.75       183\n",
      "\n",
      "Feature Importance (Odds Ratios):\n",
      "   Feature  Odds Ratio\n",
      "0     PC1  117.879510\n",
      "2     PC3    6.725676\n",
      "3     PC4    1.341192\n",
      "1     PC2    0.379883\n",
      "4     PC5    0.206994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/aa_tuesday/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [10] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Applications/anaconda3/envs/aa_tuesday/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Stratification: Datensatz in Training (75%) und Test (25%) aufteilen\n",
    "X = df.drop(columns=['Outcome'])  # Features (alle au√üer Zielvariable)\n",
    "y = df['Outcome']  # Zielvariable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Min-Max-Normalisierung\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Feature Selection (W√§hle die besten Merkmale)\n",
    "selector = SelectKBest(score_func=f_classif, k=8)  # W√§hle die 8 besten Features\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "# Principal Component Analysis (PCA zur Dimensionsreduktion)\n",
    "pca = PCA(n_components=5)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# Logistische Regression mit optimierten Hyperparametern\n",
    "model = LogisticRegression(max_iter=2000, solver='liblinear', C=1.5, penalty='l1', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluierung des Modells\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Trainingsdaten:\", X_train.shape, \"Testdaten:\", X_test.shape)\n",
    "print(\"Genauigkeit des Modells:\", accuracy)\n",
    "print(\"Klassifikationsbericht:\\n\", report)\n",
    "\n",
    "# Berechnung der Odds Ratios\n",
    "odds_ratios = np.exp(model.coef_)[0]  # Exponentiierung der Koeffizienten\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': [f'PC{i+1}' for i in range(len(odds_ratios))],\n",
    "    'Odds Ratio': odds_ratios\n",
    "}).sort_values(by='Odds Ratio', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Odds Ratios):\\n\", feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: (546, 10) Testdaten: (183, 10)\n",
      "Genauigkeit des Modells: 0.7595628415300546\n",
      "Klassifikationsbericht:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       121\n",
      "         1.0       0.68      0.55      0.61        62\n",
      "\n",
      "    accuracy                           0.76       183\n",
      "   macro avg       0.73      0.71      0.72       183\n",
      "weighted avg       0.75      0.76      0.75       183\n",
      "\n",
      "Feature Importance (Odds Ratios):\n",
      "                     Feature  Odds Ratio\n",
      "9                   Glucose   51.503143\n",
      "8      Mahalanobis_Distance    6.205647\n",
      "6                       BMI    5.950606\n",
      "4  DiabetesPedigreeFunction    4.416700\n",
      "0               Pregnancies    2.541115\n",
      "5                       Age    2.048542\n",
      "7                   Insulin    1.383210\n",
      "2             SkinThickness    1.326868\n",
      "1             BloodPressure    0.975822\n",
      "3                Unnamed: 0    0.938545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/aa_tuesday/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:111: UserWarning: Features [10] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Applications/anaconda3/envs/aa_tuesday/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Stratification: Datensatz in Training (75%) und Test (25%) aufteilen\n",
    "X = df.drop(columns=['Outcome'])  # Features (alle au√üer Zielvariable)\n",
    "y = df['Outcome']  # Zielvariable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Min-Max-Normalisierung\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Feature Selection (W√§hle die besten Merkmale)\n",
    "selector = SelectKBest(score_func=f_classif, k='all')  # Behalte alle Features, keine Einschr√§nkung\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "# RandomForest zur besseren Feature-Analyse\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importance = rf.feature_importances_\n",
    "selected_features = np.argsort(feature_importance)[-10:]  # W√§hle die 10 wichtigsten Features aus\n",
    "X_train = X_train[:, selected_features]\n",
    "X_test = X_test[:, selected_features]\n",
    "\n",
    "# Logistische Regression mit optimierten Hyperparametern\n",
    "model = LogisticRegression(max_iter=5000, solver='saga', C=1.0, penalty='l2', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluierung des Modells\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Trainingsdaten:\", X_train.shape, \"Testdaten:\", X_test.shape)\n",
    "print(\"Genauigkeit des Modells:\", accuracy)\n",
    "print(\"Klassifikationsbericht:\\n\", report)\n",
    "\n",
    "# Berechnung der Odds Ratios\n",
    "odds_ratios = np.exp(model.coef_)[0]  # Exponentiierung der Koeffizienten\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': [X.columns[i] for i in selected_features],\n",
    "    'Odds Ratio': odds_ratios\n",
    "}).sort_values(by='Odds Ratio', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Odds Ratios):\\n\", feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96c0b887-fad4-404e-9eff-7f3148abb320",
    "_uuid": "d756cba8451cf20b0742b8bc4a0d85ab6252768e"
   },
   "source": [
    "### Logistic Regression Using Standardization\n",
    "\n",
    "Do a logistic regression on data that has been standardized before. Are there differences in the model output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_tuesday",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
